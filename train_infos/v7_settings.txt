max_steps: 60 --> 30
learning_rate: 2e-4 --> 5e-5
eval_steps: 10 --> 1
save_steps: 10 --> 1


from trl import SFTConfig, SFTTrainer 
from transformers import DataCollatorForSeq2Seq

trainer = SFTTrainer(
    model = model,
    tokenizer = tokenizer,
    train_dataset = dataset["train"],   
    eval_dataset = dataset["test"],     
    dataset_text_field = "text",
    max_seq_length = max_seq_length,
    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),
    packing = False,
    args = SFTConfig(
        per_device_train_batch_size = 2,
        gradient_accumulation_steps = 4,
        warmup_steps = 5,
        max_steps = 30,
        learning_rate = 5e-5,
        logging_steps = 1,
        eval_strategy = "steps",  
        eval_steps = 1,                 
        save_strategy = "steps",         
        save_steps = 1,
        save_total_limit = 2,            
        optim = "adamw_8bit",
        weight_decay = 0.01,
        lr_scheduler_type = "linear",
        seed = 3407,
        output_dir = "outputs",
        report_to = "none",
    ),
)