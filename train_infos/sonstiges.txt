// Training v6

==((====))== Unsloth - 2x faster free finetuning | Num GPUs used = 1 \\ /| Num examples = 18 | Num Epochs = 20 | Total steps = 60 O^O/ \_/ \ Batch size per device = 2 | Gradient accumulation steps = 4 \ / Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8 "-____-" Trainable parameters = 24,313,856 of 3,237,063,680 (0.75% trained) Unsloth: Will smartly offload gradients to save VRAM! [60/60 24:47, Epoch 20/20] Step Training Loss Validation Loss 10 1.419100 1.570950 20 0.686000 1.861318 30 0.002600 3.089714 40 0.001300 3.892063 50 0.000500 3.802693 60 0.000300 3.790732 Unsloth: Not an error, but LlamaForCausalLM does not accept num_items_in_batch. Using gradient accumulation will be very slightly less accurate. Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient

// Training v7

==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 18 | Num Epochs = 10 | Total steps = 30
O^O/ \_/ \    Batch size per device = 2 | Gradient accumulation steps = 4
\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8
 "-____-"     Trainable parameters = 24,313,856 of 3,237,063,680 (0.75% trained)
 [30/30 14:55, Epoch 10/10]
Step	Training Loss	Validation Loss
1	1.449600	1.608515
2	1.584500	1.597480
3	1.164900	1.587170
4	1.332700	1.575009
5	1.584200	1.567021
6	0.928100	1.583698
7	1.089100	1.627002
8	1.450300	1.666242
9	1.164100	1.682917
10	0.983500	1.710445
11	1.307000	1.712640
12	0.722900	1.709457
13	1.123800	1.708475
14	0.794600	1.718185
15	1.136400	1.732610
16	1.045600	1.751808
17	0.732000	1.782888
18	1.000100	1.805714
19	0.848000	1.830012
20	0.753100	1.854658
21	0.902000	1.881428
22	0.697800	1.901126
23	0.670200	1.923862
24	0.933800	1.938527
25	0.547900	1.957308
26	0.896300	1.968070
27	0.083900	1.980646
28	0.654100	1.991539
29	0.711100	1.998343
30	0.046400	2.002620
Unsloth: Not an error, but LlamaForCausalLM does not accept `num_items_in_batch`.
Using gradient accumulation will be very slightly less accurate.
Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient