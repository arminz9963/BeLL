Standart Notebook von unsloth ai verwendet, mit folgenden Änderungen:

// dataset laden + mappen

from datasets import load_dataset
from unsloth.chat_templates import standardize_sharegpt

dataset = load_dataset("json", data_files="trainingsdaten3.json")

dataset = dataset["train"].train_test_split(test_size=0.1, seed=42)

dataset["train"] = standardize_sharegpt(dataset["train"])
dataset["test"]  = standardize_sharegpt(dataset["test"])

dataset["train"] = dataset["train"].map(formatting_prompts_func, batched=True)
dataset["test"]  = dataset["test"].map(formatting_prompts_func, batched=True)

// Dataset laden für alpaca

dataset = load_dataset("json", data_files="trainingsdaten3_100_v1_alpaca.json", split="train")

dataset = dataset.train_test_split(test_size=0.1, seed=42)

dataset = dataset.map(formatting_prompts_func, batched=True)

train_dataset = dataset["train"]
eval_dataset  = dataset["test"]

// Testen

print(dataset["train"][0])
print(dataset["test"][0])


// Trainingssettings

from trl import SFTConfig, SFTTrainer 
from transformers import DataCollatorForSeq2Seq

trainer = SFTTrainer(
    model = model,
    tokenizer = tokenizer,
    train_dataset = dataset["train"],   
    eval_dataset = dataset["test"],     
    dataset_text_field = "text",
    max_seq_length = max_seq_length,
    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),
    packing = False,
    args = SFTConfig(
        per_device_train_batch_size = 2,
        gradient_accumulation_steps = 4,
        warmup_steps = 5,
        max_steps = 60,
        learning_rate = 2e-4,
        logging_steps = 1,
        eval_strategy = "steps",  
        eval_steps = 10,                 # alle 10 Schritte eval
        save_strategy = "steps",         # speichern während Training
        save_steps = 10,
        save_total_limit = 2,            # nur die letzten 2 Saves behalten
        optim = "adamw_8bit",
        weight_decay = 0.01,
        lr_scheduler_type = "linear",
        seed = 3407,
        output_dir = "outputs",
        report_to = "none",
    ),
)
